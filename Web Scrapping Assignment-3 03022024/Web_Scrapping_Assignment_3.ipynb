{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ad9f58",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The \n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for \n",
    "guitars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47fea0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter product here--->guitars\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    " # Set up the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.amazon.in/')\n",
    "inputU = input('please enter product here--->')\n",
    "search_bar = driver.find_element(By.XPATH,'//input[@id=\"twotabsearchtextbox\"]')    # Finding the search bar using it's xpath\n",
    "search_bar.send_keys(inputU)       # Inputing keyword to search \n",
    "search_button = driver.find_element(By.XPATH,'//input[@id=\"nav-search-submit-button\"]')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6e4ed",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search \n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then \n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand \n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and \n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6313ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter product here--->guitars\n",
      "                                                 Brand    Price  \\\n",
      "0    Kadence A281BK Professional Acoustic Rosewood ...   ₹6,499   \n",
      "1    Kadence A281 Professional Acoustic Rosewood gu...   ₹6,499   \n",
      "2    Kadence rosewood Guitar Frontier Series, Elect...   ₹5,499   \n",
      "3    Kadence Frontier guitar with Online Guitar lea...   ₹4,999   \n",
      "4    Yamaha F280 Acoustic Rosewood Guitar (Natural,...   ₹7,400   \n",
      "..                                                 ...      ...   \n",
      "164        Yamaha FG 800 Folk Acoustic Guitar, Natural  ₹12,399   \n",
      "165  Kadence Frontier Series 40 Incheses wood Semi ...  ₹17,999   \n",
      "166       Vault ST1 Premium Electric Guitar - Sunburst  ₹20,499   \n",
      "167  Kadence Guitar Slowhand Series Premium Electri...   ₹5,499   \n",
      "168           Intern INT-38C Acoustic Guitar Kit (Red)   ₹8,599   \n",
      "\n",
      "                   Expected Delivery  \\\n",
      "0    Get it by Saturday, 10 February   \n",
      "1    Get it by Saturday, 10 February   \n",
      "2     Get it by Tomorrow, 9 February   \n",
      "3     Get it by Tomorrow, 9 February   \n",
      "4    Get it by Saturday, 10 February   \n",
      "..                               ...   \n",
      "164  Get it by Saturday, 10 February   \n",
      "165   Get it by Tuesday, 13 February   \n",
      "166  Get it by Saturday, 10 February   \n",
      "167  Get it by Saturday, 10 February   \n",
      "168                                    \n",
      "\n",
      "                                           Product URL  \n",
      "0    https://www.amazon.in/sspa/click?ie=UTF8&spc=M...  \n",
      "1    https://www.amazon.in/sspa/click?ie=UTF8&spc=M...  \n",
      "2    https://www.amazon.in/sspa/click?ie=UTF8&spc=M...  \n",
      "3    https://www.amazon.in/sspa/click?ie=UTF8&spc=M...  \n",
      "4    https://www.amazon.in/Yamaha-F280-Acoustic-Gui...  \n",
      "..                                                 ...  \n",
      "164  https://www.amazon.in/Kadence-Frontier-Acousti...  \n",
      "165  https://www.amazon.in/Vault-ST1-Premium-Electr...  \n",
      "166  https://www.amazon.in/Kadence-Slowhand-Acousti...  \n",
      "167  https://www.amazon.in/Intern-INT-38C-Acoustic-...  \n",
      "168  https://www.amazon.in/FX280-acoustic-Dreadnoug...  \n",
      "\n",
      "[169 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.amazon.in/')\n",
    "inputU = input('please enter product here--->')\n",
    "search_bar = driver.find_element(By.XPATH, '//input[@id=\"twotabsearchtextbox\"]')    # Finding the search bar using its xpath\n",
    "search_bar.send_keys(inputU)       # Inputting keyword to search \n",
    "search_button = driver.find_element(By.XPATH, '//input[@id=\"nav-search-submit-button\"]')    # Finding the xpath of the search button\n",
    "search_button.click()        # Clicking the search button\n",
    "\n",
    "Brands = []\n",
    "Prices = []\n",
    "Expected_Delivery = []\n",
    "Product_links = []\n",
    "\n",
    "start = 0\n",
    "end = 3  # Increase the number of pages you want to scrape\n",
    "\n",
    "for page in range(start, end):\n",
    "    # Scraping brands\n",
    "    br = driver.find_elements(By.XPATH, '//span[@class=\"a-size-base-plus a-color-base a-text-normal\"]')\n",
    "    for i in br:\n",
    "        if i.text is None :\n",
    "            Brands.append(\"--\")\n",
    "        else:\n",
    "            Brands.append(i.text)\n",
    "    \n",
    "    # Scraping prices\n",
    "    pri = driver.find_elements(By.XPATH, '//span[@class=\"a-price\"]')\n",
    "    for i in pri:\n",
    "        if i.text is None :\n",
    "            Prices.append(\"--\")\n",
    "        else:\n",
    "            Prices.append(i.text)\n",
    "    \n",
    "    # Scraping expected delivery\n",
    "    exp_del = driver.find_elements(By.XPATH, '//div[@class=\"a-row s-align-children-center\"]')\n",
    "    for i in exp_del:\n",
    "        if i.text is None :\n",
    "            Expected_Delivery.append(\"--\")\n",
    "        else:\n",
    "            Expected_Delivery.append(i.text)\n",
    "    \n",
    "    # Scraping product links\n",
    "    product_links = driver.find_elements(By.XPATH, '//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-4\"]/a')\n",
    "    for link in product_links:\n",
    "        product_url = link.get_attribute(\"href\")\n",
    "        if product_url is None or product_url.strip() == \"\":\n",
    "            Product_links.append(\"--\")\n",
    "        else:\n",
    "            Product_links.append(product_url)\n",
    "        \n",
    "    # Clicking next page button\n",
    "    next_button = driver.find_element(By.XPATH, '//a[@class=\"s-pagination-item s-pagination-button\"]')\n",
    "    next_button.click()    \n",
    "    time.sleep(5)\n",
    "\n",
    "# Ensure all lists have the same length\n",
    "min_length = min(len(Brands), len(Prices), len(Expected_Delivery), len(Product_links))\n",
    "Brands = Brands[:min_length]\n",
    "Prices = Prices[:min_length]\n",
    "Expected_Delivery = Expected_Delivery[:min_length]\n",
    "Product_links = Product_links[:min_length]\n",
    "\n",
    "# Creating DataFrame\n",
    "data = {\n",
    "    \"Brand\": Brands,\n",
    "    \"Price\": Prices,\n",
    "    \"Expected Delivery\": Expected_Delivery,\n",
    "    \"Product URL\": Product_links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Saving to CSV\n",
    "df.to_csv(\"amazon_products.csv\", index=False)\n",
    "\n",
    "# Printing DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f644eda",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10 \n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4077072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 images for 'fruits' downloaded successfully to 'C:\\Users\\HP\\downloaded_images\\fruits'.\n",
      "10 images for 'cars' downloaded successfully to 'C:\\Users\\HP\\downloaded_images\\cars'.\n",
      "10 images for 'Machine Learning' downloaded successfully to 'C:\\Users\\HP\\downloaded_images\\Machine Learning'.\n",
      "10 images for 'Guitar' downloaded successfully to 'C:\\Users\\HP\\downloaded_images\\Guitar'.\n",
      "10 images for 'Cakes' downloaded successfully to 'C:\\Users\\HP\\downloaded_images\\Cakes'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import urllib.request  # Import urllib.request for downloading images\n",
    "\n",
    "def scrape_images(keyword, num_images=10):\n",
    "    driver = webdriver.Chrome()  # Make sure you have ChromeDriver installed and its path set in system environment variables.\n",
    "    \n",
    "    try:\n",
    "        # Open Google Images\n",
    "        driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "        # Find and input the search bar\n",
    "        search_bar = driver.find_element(\"name\", \"q\")\n",
    "        search_bar.send_keys(keyword)\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "        # Wait for the search results to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Scroll down to load more images (adjust the range as needed)\n",
    "        for _ in range(3):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "\n",
    "        # Find and scrape image URLs using an  XPath expression\n",
    "        image_elements = driver.find_elements(By.XPATH, '//img[@class=\"rg_i Q4LuWd\"]')\n",
    "        image_urls = [element.get_attribute('src') for element in image_elements[:num_images]]\n",
    "\n",
    "        # Download images to a folder\n",
    "        download_path = os.path.join(os.getcwd(), \"downloaded_images\", keyword)\n",
    "        os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "        for i, url in enumerate(image_urls):\n",
    "            image_path = os.path.join(download_path, f\"{keyword}_{i + 1}.jpg\")\n",
    "            urllib.request.urlretrieve(url, image_path)\n",
    "\n",
    "        print(f\"{num_images} images for '{keyword}' downloaded successfully to '{download_path}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "for keyword in keywords:\n",
    "    scrape_images(keyword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddf0deb",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand \n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, \n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the \n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1e1cd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the smartphone you want to search for on Flipkart: Oneplus Nord, pixel 4A\n",
      "                                    Brand Name                     Rom  \\\n",
      "0   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)                       -   \n",
      "1   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)                       -   \n",
      "2   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)   8 GB RAM | 128 GB ROM   \n",
      "3   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)   8 GB RAM | 128 GB ROM   \n",
      "4   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)    6 GB RAM | 64 GB ROM   \n",
      "5   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)  12 GB RAM | 256 GB ROM   \n",
      "6   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)   8 GB RAM | 128 GB ROM   \n",
      "7   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)  12 GB RAM | 256 GB ROM   \n",
      "8   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)    6 GB RAM | 64 GB ROM   \n",
      "9   OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)   8 GB RAM | 128 GB ROM   \n",
      "10  OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)   6 GB RAM | 128 GB ROM   \n",
      "11  OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)                       -   \n",
      "12  OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)   8 GB RAM | 128 GB ROM   \n",
      "13  OnePlus Nord CE 2 5G (Bahama Blue, 128 GB)                       -   \n",
      "\n",
      "                                       Camera  \\\n",
      "0                                           -   \n",
      "1                                           -   \n",
      "2                            64MP Rear Camera   \n",
      "3   48MP + 8MP | 32MP + 8MP Dual Front Camera   \n",
      "4   48MP + 8MP | 32MP + 8MP Dual Front Camera   \n",
      "5   48MP + 8MP | 32MP + 8MP Dual Front Camera   \n",
      "6   48MP + 8MP | 32MP + 8MP Dual Front Camera   \n",
      "7   48MP + 8MP | 32MP + 8MP Dual Front Camera   \n",
      "8   48MP + 8MP | 32MP + 8MP Dual Front Camera   \n",
      "9                            64MP Rear Camera   \n",
      "10       64MP Rear Camera | 16MP Front Camera   \n",
      "11                                          -   \n",
      "12                           64MP Rear Camera   \n",
      "13                                          -   \n",
      "\n",
      "                             Display Size Battery Capacity    Price  \\\n",
      "0                                       -          ₹21,999  ₹21,999   \n",
      "1                                       -          ₹21,999  ₹21,999   \n",
      "2            16.33 cm (6.43 inch) Display          ₹21,999  ₹21,999   \n",
      "3   16.36 cm (6.44 inch) Full HD+ Display          ₹21,999  ₹21,999   \n",
      "4   16.36 cm (6.44 inch) Full HD+ Display          ₹21,999  ₹21,999   \n",
      "5   16.36 cm (6.44 inch) Full HD+ Display          ₹21,999  ₹21,999   \n",
      "6   16.36 cm (6.44 inch) Full HD+ Display          ₹21,999  ₹21,999   \n",
      "7   16.36 cm (6.44 inch) Full HD+ Display          ₹21,999  ₹21,999   \n",
      "8   16.36 cm (6.44 inch) Full HD+ Display          ₹21,999  ₹21,999   \n",
      "9            16.33 cm (6.43 inch) Display          ₹21,999  ₹21,999   \n",
      "10           16.33 cm (6.43 inch) Display          ₹21,999  ₹21,999   \n",
      "11                                      -          ₹21,999  ₹21,999   \n",
      "12           16.33 cm (6.43 inch) Display          ₹21,999  ₹21,999   \n",
      "13                                      -          ₹21,999  ₹21,999   \n",
      "\n",
      "                                          Product URL  \n",
      "0   https://www.flipkart.com/mobiles-accessories/p...  \n",
      "1   https://www.flipkart.com/buying-guide/mobiles?...  \n",
      "2   https://www.flipkart.com/oneplus-nord-ce-2-5g-...  \n",
      "3   https://www.flipkart.com/oneplus-nord-blue-mar...  \n",
      "4   https://www.flipkart.com/oneplus-nord-gray-ony...  \n",
      "5   https://www.flipkart.com/oneplus-nord-blue-mar...  \n",
      "6   https://www.flipkart.com/oneplus-nord-gray-ony...  \n",
      "7   https://www.flipkart.com/oneplus-nord-gray-ony...  \n",
      "8   https://www.flipkart.com/oneplus-nord-blue-mar...  \n",
      "9   https://www.flipkart.com/oneplus-nord-ce-2-5g-...  \n",
      "10  https://www.flipkart.com/oneplus-nord-ce-2-5g-...  \n",
      "11                                                  -  \n",
      "12  https://www.flipkart.com/oneplus-nord-ce-2-5g-...  \n",
      "13                                                  -  \n"
     ]
    }
   ],
   "source": [
    "def scrape_flipkart(search_query):\n",
    "    # Set up the Chrome driver\n",
    "    driver = webdriver.Chrome()\n",
    "    # Navigate to Flipkart search results page\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow time for the page to load\n",
    "\n",
    "    # Initialize lists to store scraped data\n",
    "    brand_names = []\n",
    "    roms = []\n",
    "    cameras = []\n",
    "    display_sizes = []\n",
    "    battery_capacities = []\n",
    "    prices = []\n",
    "    product_urls = []\n",
    "\n",
    "    # Extract data from each product on the page\n",
    "    products = driver.find_elements(By.XPATH,'//div[@class=\"_1AtVbE col-12-12\"]')\n",
    "    for product in products:\n",
    "        try:\n",
    "            brand_name = product.find_element(By.XPATH,'//div[@class=\"_4rR01T\"]').text\n",
    "        except:\n",
    "            brand_name = '-'\n",
    "\n",
    "\n",
    "        try:\n",
    "            rom = product.find_element(By.XPATH,'.//li[contains(text(), \"ROM\")]').text\n",
    "        except:\n",
    "            rom = '-'\n",
    "\n",
    "        try:\n",
    "            primary_camera = product.find_element(By.XPATH,'.//li[contains(text(), \"Camera\")]').text\n",
    "        except:\n",
    "            primary_camera = '-'\n",
    "\n",
    "      \n",
    "        try:\n",
    "            display_size = product.find_element(By.XPATH,'.//li[contains(text(), \"Display\")]').text\n",
    "        except:\n",
    "            display_size = '-'\n",
    "\n",
    "        try:\n",
    "            battery_capacity = product.find_element(By.XPATH,'.//li[contains(text(), \"Battery\")]').text\n",
    "        except:\n",
    "            battery_capacity = '-'\n",
    "\n",
    "        try:\n",
    "            price =  battery_capacity = product.find_element(By.XPATH,'//div[@class=\"_30jeq3 _1_WHN1\"]').text\n",
    "        except:\n",
    "            price = '-'\n",
    "\n",
    "        try:\n",
    "            product_url = product.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "        except:\n",
    "            product_url = '-'\n",
    "\n",
    "        # Append the extracted data to the lists\n",
    "        brand_names.append(brand_name)\n",
    "        roms.append(rom)\n",
    "        cameras.append(primary_camera)\n",
    "        display_sizes.append(display_size)\n",
    "        battery_capacities.append(battery_capacity)\n",
    "        prices.append(price)\n",
    "        product_urls.append(product_url)\n",
    "\n",
    "    # Create a dataframe from the lists\n",
    "    data = {\n",
    "        'Brand Name': brand_names,\n",
    "        'Rom': roms,\n",
    "        'Camera': cameras,\n",
    "        'Display Size': display_sizes,\n",
    "        'Battery Capacity': battery_capacities,\n",
    "        'Price': prices,\n",
    "        'Product URL': product_urls\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    df.to_csv('flipkart_smartphones.csv', index=False)\n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    scrape_flipkart(search_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c680b3",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237c031f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location(Dharmavaram, Sri Sathyasai, Andhra Pradesh, (14.41328, 77.71049, 0.0))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geopy.geocoders import ArcGIS\n",
    "def scrape_geospatial_coordinates (city):\n",
    "    nom=ArcGIS()\n",
    "    coordinates =nom.geocode(city)\n",
    "    return coordinates\n",
    "scrape_geospatial_coordinates('dharmavaram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711af830",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312beb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lenovo IdeaPad Gaming 3 15IAH7', 'Lenovo IdeaPad Gaming 3i', 'Alienware x14 Gaming Laptop', 'MSI Gaming Raider', 'Lenovo Legion 7', 'Dell 15 (2021) i5-10200H']\n",
      "['★★★★★', '★★★★★', '★★★★★']\n",
      "['₹79,831.00', '₹174,990.00', '₹328,800.00']\n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the website\n",
    "driver.get('https://www.digit.in/')\n",
    "\n",
    "# Search for gaming laptops\n",
    "search_bar = driver.find_element(By.XPATH,'//input[@id=\"woocommerce-product-search-field-0\"]')\n",
    "search_bar.send_keys('gaming laptops')\n",
    "search_bar.submit()\n",
    "\n",
    "Brands=[]\n",
    "Ratings=[]\n",
    "Price=[]\n",
    "\n",
    "\n",
    "# Wait for the search results to load\n",
    "time.sleep(2)\n",
    "\n",
    "br=driver.find_elements(By.XPATH,'//h3[@class=\" text-clamp text-clamp-2\"]')\n",
    "len(br)\n",
    "\n",
    "for i in br:\n",
    "    if i.text is None :\n",
    "        \n",
    "        Brands.append(\"--\") \n",
    "    else:\n",
    "        Brands.append(i.text)\n",
    "#print(len(Investment_Type),Investment_Type)\n",
    "                \n",
    "print(Brands)\n",
    "\n",
    "\n",
    "rat=driver.find_elements(By.XPATH,'//div[@class=\"rh_woo_star\"]')\n",
    "for i in rat:\n",
    "    if i.text is None :\n",
    "         Ratings.append(\"--\") \n",
    "    else:\n",
    "        Ratings.append(i.text)\n",
    "        \n",
    "print(Ratings)\n",
    "\n",
    "\n",
    "pri=driver.find_elements(By.XPATH,'//span[@class=\"price\"]')\n",
    "for i in pri:\n",
    "    if i.text is None :\n",
    "         Price.append(\"--\") \n",
    "    else:\n",
    "        Price.append(i.text)\n",
    "print(Price)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096c7a8",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: \n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aac4b959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank                      Name Net worth Age    Citizenship  \\\n",
      "0      1  Bernard Arnault & family    $211 B  74         France   \n",
      "1      2                 Elon Musk    $180 B  51  United States   \n",
      "2      3                Jeff Bezos    $114 B  59  United States   \n",
      "3      4             Larry Ellison    $107 B  78  United States   \n",
      "4      5            Warren Buffett    $106 B  92  United States   \n",
      "..   ...                       ...       ...  ..            ...   \n",
      "195  195               Jin Baofang    $9.6 B  70          China   \n",
      "196  195        Luo Liguo & family    $9.6 B  67          China   \n",
      "197  195              Marijke Mars    $9.6 B  58  United States   \n",
      "198  195               Pamela Mars    $9.6 B  62  United States   \n",
      "199  195              Valerie Mars    $9.6 B  64  United States   \n",
      "\n",
      "                 Source Industry  \n",
      "0                  LVMH           \n",
      "1         Tesla, SpaceX           \n",
      "2                Amazon           \n",
      "3                Oracle           \n",
      "4    Berkshire Hathaway           \n",
      "..                  ...      ...  \n",
      "195        Solar panels           \n",
      "196           Chemicals           \n",
      "197     Candy, pet food           \n",
      "198     Candy, pet food           \n",
      "199     Candy, pet food           \n",
      "\n",
      "[200 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Find the table element using its XPath\n",
    "table_xpath = \"//div[@class='Table_tableWrapper__fGr5G']\"\n",
    "table = driver.find_element(By.XPATH, table_xpath)\n",
    "\n",
    "# Find all rows of the table using their XPath\n",
    "\n",
    "cell_xpath = '//div[@class=\"TableRow_cell__db-hv Table_cell__houv9\"]'\n",
    "\n",
    "# Find the table element using its XPath\n",
    "table = driver.find_element(By.XPATH, table_xpath)\n",
    "\n",
    "# Find all cells containing required information within the table\n",
    "cells = table.find_elements(By.XPATH, cell_xpath)\n",
    "\n",
    "data = []\n",
    "\n",
    "data = []\n",
    "\n",
    "# Iterate through cells and extract data for \"Rank\", \"Name\", \"Net worth\", \"Age\", and \"Source\"\n",
    "data = []\n",
    "\n",
    "# Iterate through cells and extract data for \"Rank\", \"Name\", \"Net worth\", \"Age\", and \"Source\"\n",
    "data = []\n",
    "\n",
    "# Iterate through cells and extract data for \"Rank\", \"Name\", \"Net worth\", \"Age\", \"Citizenship\", \"Source\", and \"Industry\"\n",
    "for i in range(0, len(cells), 7):\n",
    "    # Check if there are enough cells remaining\n",
    "    if len(cells) < i + 7:\n",
    "        break\n",
    "    \n",
    "    # Extract text from specific cells (Rank, Name, Net worth, Age, Citizenship, Source, Industry)\n",
    "    rank = cells[i].text\n",
    "    name = cells[i + 1].text\n",
    "    net_worth = cells[i + 2].text\n",
    "    age = cells[i + 3].text\n",
    "    citizenship = cells[i + 4].text\n",
    "    source = cells[i + 5].text\n",
    "    industry = cells[i + 6].text\n",
    "    \n",
    "    # Append data to the list\n",
    "    data.append({\n",
    "        \"Rank\": rank,\n",
    "        \"Name\": name,\n",
    "        \"Net worth\": net_worth,\n",
    "        \"Age\": age,\n",
    "        \"Citizenship\": citizenship,\n",
    "        \"Source\": source,\n",
    "        \"Industry\": industry\n",
    "    })\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "    \n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bceca7",
   "metadata": {},
   "source": [
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted \n",
    "from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d9ce2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beautiful  always in my ,', 'Love this song ', 'beautiful', '', 'Botal Ka pata chala', 'Wow wow wow ', '', '', 'Love।song।', 'Kya mast song hai nice  ', 'So sweet songs', '', '', '', '', 'Hi all fb', 'How are fb', 'Hi', 'hello', 'Kon kon Instagram per sun kar Aya hai', '', ' I love this song ', 'X', '', 'E', 'Ye mtrcd addd Wale Santi se gana sunne Nehi deraha', '', '', '']\n",
      "['7', '10', '4', '12', '15', '', '11', '5', '', '2', '31', '1', '1', '1', '2', '', '1', '5', '', '', '1', '1', '', '5', '1', '', '2', '']\n",
      "['Ocean of Vibes', '', '', '', '7 days ago', '4 days ago', '12 days ago', '11 days ago', '6 days ago', '2 days ago', '11 days ago', '12 days ago', '22 hours ago', '4 days ago', '7 days ago', '2 days ago', '7 days ago', '6 days ago', '1 day ago', '3 days ago', '3 days ago', '13 days ago', '5 days ago', '1 day ago', '5 days ago', '7 days ago', '4 days ago', '2 weeks ago', '8 days ago', '18 hours ago', '4 days ago', '4 days ago']\n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()  # Replace with the path to your WebDriver executable\n",
    "\n",
    "# Open the YouTube video\n",
    "video_url = 'https://www.youtube.com/watch?v=1Xbv6JC_Zr4'  # Replace with the URL of the YouTube video\n",
    "driver.get(video_url)\n",
    "\n",
    "# Scroll to load comments\n",
    "scroll_pause_time = 10  # Adjust the pause time as needed\n",
    "scrolls = 1  # Adjust the number of scrolls as needed\n",
    "\n",
    "for _ in range(scrolls):\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n",
    "\n",
    "# Extract comments, upvotes, and time\n",
    "Comments=[]\n",
    "comments = driver.find_elements(By.XPATH,'//div[@class=\"style-scope ytd-expander\"]')\n",
    "for i in comments:\n",
    "    Comments.append(i.text)\n",
    "print(Comments)\n",
    "\n",
    "\n",
    "Upvotes=[]\n",
    "upvotes = driver.find_elements(By.XPATH,'//span[@id=\"vote-count-middle\"]')\n",
    "scroll_pause_time = 10  # Adjust the pause time as needed\n",
    "scrolls = 1  # Adjust the number of scrolls as needed\n",
    "for i in upvotes:\n",
    "    Upvotes.append(i.text)\n",
    "print(Upvotes)\n",
    "\n",
    "\n",
    "Times=[]\n",
    "times = driver.find_elements(By.XPATH,'//a[@class=\"yt-simple-endpoint style-scope yt-formatted-string\"]')\n",
    "for i in times:\n",
    "    Times.append(i.text)\n",
    "print(Times)\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605c369",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in \n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall \n",
    "reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd3c122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Wombat's City Hostel London\", 'Palmers Lodge - Swiss Cottage', \"St Christopher's Village\", 'NX London Hostel', 'Urbany Hostel London', 'Onefam Notting Hill', 'Generator London', 'Onefam Waterloo', 'Astor Hyde Park', 'Safestay London Elephant & Castle', 'Astor Museum Inn', 'No.8 Seven Sisters', 'No.8 Willesden Hostel London', 'Clink261', 'Smart Russell Square Hostel', 'Safestay London Kensington Holland Park', 'YHA London Oxford Street', 'Smart Hyde Park Inn Hostel', 'Destinations Hostels @ The Gallery', 'Queen Elizabeth Chelsea', 'Hostelle - women only hostel London', 'Prime Backpackers Angel', 'Kabannas London St Pancras', 'London Backpackers', 'Smart Camden Inn Hostel', 'Astor Kensington', \"St Christopher's Hammersmith\", 'Astor Victoria', 'Phoenix Hostel', 'Barmy Badger Backpackers']\n",
      "['- 3.6km from city centre', '- 6.5km from city centre', '- 1.8km from city centre', '- 6.1km from city centre', '- 5.4km from city centre', '- 5.5km from city centre', '- 3km from city centre', '- 0.7km from city centre', '- 4.3km from city centre', '- 1.7km from city centre', '- 2.2km from city centre', '- 9km from city centre', '- 10km from city centre', '- 3.2km from city centre', '- 2.6km from city centre', '- 5.8km from city centre', '- 2.1km from city centre', '- 5km from city centre', '- 1.7km from city centre', '- 5.7km from city centre', '- 5.1km from city centre', '- 3.6km from city centre', '- 3.3km from city centre', '- 11.9km from city centre', '- 4.4km from city centre', '- 4.9km from city centre', '- 7.5km from city centre', '- 1.8km from city centre', '- 4.2km from city centre', '- 5.5km from city centre']\n",
      "['9.1', '8.8', '7.8', '9.1', '8.8', '7.8', '8.9', '9.5', '9.6', '7.5', '9.6', '8.2', '7.2', '8.3', '7.7', '7.2', '7.9', '7.7', '7.0', '9.0', '7.8', '9.1', '7.4', '8.8', '8.5', '8.2', '8.1', '8.3', '8.5', '7.7', '7.3', '6.3', '9.1']\n",
      "['(15894)', '(15971)', '(12871)', '(2146)', '(1125)', '(2588)', '(8081)', '(131)', '(12327)', '(5278)', '(9455)', '(4178)', '(5201)', '(514)', '(10205)', '(1781)', '(4703)', '(6800)', '(316)', '(3685)', '(115)', '(871)', '(2977)', '(4567)', '(3143)', '(6594)', '(4389)', '(15097)', '(4624)', '(2041)']\n",
      "['-30% Privates From ₹18,899 ₹13,229', 'No Dorms Available', 'No Privates Available', '-26% Dorms From ₹3,331 ₹2,465', 'No Privates Available', '-10% Dorms From ₹1,849 ₹1,664', '-20% Privates From ₹16,083 ₹12,866', '-15% Dorms From ₹3,424 ₹2,910', 'Privates From ₹12,469', 'Dorms From ₹3,318', 'Privates From ₹11,524', 'Dorms From ₹3,260', 'Privates From ₹11,695', 'Dorms From ₹3,143', 'No Privates Available', 'Dorms From ₹4,322', 'No Privates Available', '-10% Dorms From ₹2,305 ₹2,074', 'Privates From ₹20,078', 'Dorms From ₹2,136', 'Privates From ₹11,663', 'Dorms From ₹3,045', 'No Privates Available', 'Dorms From ₹1,130', 'Privates From ₹3,347', 'Dorms From ₹1,007', 'Privates From ₹9,617', 'Dorms From ₹2,558', 'No Privates Available', 'Dorms From ₹1,829', 'No Privates Available', 'Dorms From ₹1,781', 'Privates From ₹12,222', 'No Dorms Available', 'Privates From ₹10,544', 'Dorms From ₹1,672', 'No Privates Available', 'Dorms From ₹3,801', 'No Privates Available', 'Dorms From ₹2,029', 'No Privates Available', 'Dorms From ₹2,898', 'No Privates Available', 'Dorms From ₹3,210', 'No Privates Available', '-20% Dorms From ₹4,562 ₹3,650', 'No Privates Available', 'Dorms From ₹1,467', 'No Privates Available', 'Dorms From ₹2,175', '-10% Privates From ₹10,755 ₹9,680', '-10% Dorms From ₹1,956 ₹1,760', 'No Privates Available', '-10% Dorms From ₹2,082 ₹1,873', 'Privates From ₹10,895', 'Dorms From ₹1,956', '-5% Privates From ₹6,003 ₹5,703', '-5% Dorms From ₹1,603 ₹1,523', 'Privates From ₹6,984', 'Dorms From ₹2,968']\n"
     ]
    }
   ],
   "source": [
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "# Define the URL you want to scrape\n",
    "url = \"https://www.hostelworld.com/pwa/wds/s?q=London,%20England&country=London&city=London&type=city&id=3&from=2024-02-09&to=2024-02-12&guests=2&page=1\"\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5) \n",
    "Hostel_info = []\n",
    "hostel_name = driver.find_elements(By.XPATH, '//div[@class=\"property-name\"]/span')\n",
    "for i in hostel_name:\n",
    "    Hostel_info.append(i.text)\n",
    "    \n",
    "print(Hostel_info)\n",
    "\n",
    "# Get distance from city centre\n",
    "Distance=[]\n",
    "distance = driver.find_elements(By.XPATH, '//span[@class=\"distance-description\"]')\n",
    "for i in distance:\n",
    "    Distance.append(i.text)\n",
    "    \n",
    "print(Distance)\n",
    "\n",
    " # Get ratings\n",
    "Ratings=[]\n",
    "ratings = driver.find_elements(By.XPATH, '//span[@class=\"number\"]')\n",
    "for i in ratings:\n",
    "    Ratings.append(i.text)\n",
    "print(Ratings)\n",
    "\n",
    " # Get total reviews\n",
    "Total_reviews=[]\n",
    "total_reviews = driver.find_elements(By.XPATH, '//div[@class=\"review\"]')\n",
    "for i in total_reviews:\n",
    "    Total_reviews.append(i.text)\n",
    "print(Total_reviews)\n",
    "\n",
    "\n",
    "# Get privates from price\n",
    "privates_price_dorms_price = driver.find_elements(By.XPATH,'//div[@class=\"property-accommodation-prices\"]/div')\n",
    "Privates_price_dorms_price_cleaned = []\n",
    "\n",
    "for i in privates_price_dorms_price:\n",
    "    price_text = i.text.replace('\\n', ' ')  # Remove '\\n' characters\n",
    "    Privates_price_dorms_price_cleaned.append(price_text)\n",
    "\n",
    "print(Privates_price_dorms_price_cleaned)\n",
    "\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
